{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hometask 7 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача: \n",
    "запустить модель LDA и Gibbs Sampling с числов тегов 20. Вывести топ-10 слов по каждому тегу. Соотнести полученные теги с тегами из датасета, сделать выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать усеченный словарь для ускорения алгоритма, т.к. наша задача найти наиболее часто встречающиеся слова в каждой из тем (тэгов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=25,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=frozenset({'might', 'always', 'its', 'whole', 'about', 'another', 'cry', 'who', 'rather', 'behind', 'even', 'some', 'name', 'an', 'all', 'few', 'couldnt', 'very', 'anyone', 'on', 'must', 'moreover', 'cant', 'whether', 'before', 'hasnt', 'nobody', 'full', 'keep', 'being', 'with', 'thereaft..., 'own', 'during', 'along', 'will', 'within', 'such', 'between', 'have', 'now', 'several', 'third'}),\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words=ENGLISH_STOP_WORDS,\n",
    "                             analyzer='word', binary=True, min_df = 25)\n",
    "vectorizer.fit(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4983"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сэмплирование по данному распределению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_weight(weights):\n",
    "    #return the position with respect to its weight\n",
    "    weights_normed = np.sort(weights) / np.sum(weights)\n",
    "    weights_bounded = np.cumsum(weights_normed)\n",
    "\n",
    "    rand = np.random.rand()\n",
    "    for i in range(len(weights)):\n",
    "        if(rand < weights_bounded[i]):\n",
    "            rand = np.argsort(weights)[i]\n",
    "            break;\n",
    "    return rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "### main part\n",
    "word_to_topic = np.zeros(len(vectorizer.vocabulary_), dtype = int) # z     -- in which topic the word\n",
    "num_topic = np.zeros(len(newsgroups_train.target_names))           # n_k   -- number of words in topic k\n",
    "num_topic_word = np.zeros((len(newsgroups_train.target_names), len(vectorizer.vocabulary_)))   # <-- n_k,w\n",
    "                                                                   # n_k,w -- amount of times word w was assigned with topic k\n",
    "num_text_topic = np.zeros((len(newsgroups_train.data), len(newsgroups_train.target_names)))    # <-- n_d,k\n",
    "                                                                   # n_d,k -- number of words assigned with topic k in text d\n",
    "\n",
    "alpha = np.zeros(len(newsgroups_train.target_names))               # alpha ~ distribution of topics over texts \n",
    "beta = np.zeros((len(newsgroups_train.target_names), len(vectorizer.vocabulary_)))  \n",
    "                                                                   # beta  ~ distribution of topics over words\n",
    "\n",
    "\n",
    "# random connect words with topics\n",
    "for i in range(len(vectorizer.vocabulary_)):       \n",
    "    # because len(newsgroups_train.target_names) =  20\n",
    "    word_to_topic[i] = generate_with_weight(np.full(20, 1/20))\n",
    "\n",
    "# update counters    \n",
    "for i in range(len(newsgroups_train.data)):\n",
    "    alpha[newsgroups_train.target[i]] = alpha[newsgroups_train.target[i]] + 1\n",
    "    text = newsgroups_train.data[i]\n",
    "    beta[newsgroups_train.target[i]] = beta[newsgroups_train.target[i]] + vectorizer.transform([text])\n",
    "    \n",
    "    x = np.resize(vectorizer.transform([text]).toarray(), len(vectorizer.vocabulary_))\n",
    "    b = np.argwhere(x)\n",
    "    c = word_to_topic[b]\n",
    "    for j in range(len(num_topic)):\n",
    "        num_text_topic[i, j] = len(c[(c == j)])\n",
    "        num_topic[j] = num_topic[j] + len(c[(c == j)])\n",
    "    text_transformed = vectorizer.inverse_transform(vectorizer.transform([text]))[0]\n",
    "    for j in range(len(text_transformed)):\n",
    "        word = vectorizer.vocabulary_.get(text_transformed[j])\n",
    "        num_topic_word[word_to_topic[word], word] = num_topic_word[word_to_topic[word], word] + 1\n",
    "#num_corpus_topic = np.sum(num_text_topic, axis = 0)\n",
    "\n",
    "\n",
    "## Algorithm\n",
    "for count in range(150):                                            # will go several times through the corpus for stability\n",
    "    for i in range(len(newsgroups_train.data)):\n",
    "        text = newsgroups_train.data[i]\n",
    "        text_transformed = vectorizer.inverse_transform(vectorizer.transform([text]))[0]  # --[0] because of [text]\n",
    "        for j in range(len(text_transformed)):\n",
    "            # change counters\n",
    "            word = vectorizer.vocabulary_.get(text_transformed[j]) # word index in vocabulary_\n",
    "            topic = word_to_topic[word]\n",
    "            num_text_topic[i, topic] = num_text_topic[i, topic] - 1\n",
    "            num_topic[topic] = num_topic[topic] - 1\n",
    "            num_topic_word[topic, word] = num_topic_word[topic, word] - 1\n",
    "            #\n",
    "            p = np.zeros(len(num_topic))\n",
    "            for k in range(len(num_topic)):\n",
    "                # formula:  p[k] = (n_d,k + alpha[k]) * (n[k,w_i] + beta_w_i) / (n_k + beta_sum)\n",
    "                p[k] = (num_text_topic[i, k] + alpha[k]) * (num_topic_word[k, word] + beta[k, word]) / (num_topic[k] + \n",
    "                                                                                                        np.sum(beta[k]))\n",
    "                #\n",
    "            # sample new smth\n",
    "            topic = generate_with_weight(np.abs(p))\n",
    "            word_to_topic[word] = topic\n",
    "            # change counters back\n",
    "            num_text_topic[i, topic] = num_text_topic[i, topic] + 1\n",
    "            num_topic[topic] = num_topic[topic] + 1\n",
    "            num_topic_word[topic, word] = num_topic_word[topic, word] + 1\n",
    "            #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 in Topic = alt.atheism\n",
      "\n",
      "way doesn read actually yes program remember instead news \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = comp.graphics\n",
      "\n",
      "want 24 past wants mark reasonable wondering issues united \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = comp.os.ms-windows.misc\n",
      "\n",
      "things kind left nice advance type play argument strong \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = comp.sys.ibm.pc.hardware\n",
      "\n",
      "using says stuff version understand area live standard consider \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = comp.sys.mac.hardware\n",
      "\n",
      "long thing world second hand john gets aren children \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = comp.windows.x\n",
      "\n",
      "good really 10 high software came national single purpose \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = misc.forsale\n",
      "\n",
      "information state note original goes access looks assume expect \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = rec.autos\n",
      "\n",
      "think away large today 50 days article car university \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = rec.motorcycles\n",
      "\n",
      "right old god run ask appreciated 21 test easily \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = rec.sport.baseball\n",
      "\n",
      "help better come set true email home reading allow \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = rec.sport.hockey\n",
      "\n",
      "time let seen mean comes 14 started 17 games \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = sci.crypt\n",
      "\n",
      "make great day bad simply similar address talk fast \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = sci.electronics\n",
      "\n",
      "new need order change memory wanted include sound needed \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = sci.med\n",
      "\n",
      "ve years point following available doing times tried working \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = sci.space\n",
      "\n",
      "problem question number lot non given drive 30 running \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = soc.religion.christian\n",
      "\n",
      "sure man feel sun board coming hold follow 32 \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = talk.politics.guns\n",
      "\n",
      "course try big hope certainly posted considered model complete \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = talk.politics.mideast\n",
      "\n",
      "far post unless soon ones stop rights various center \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = talk.politics.misc\n",
      "\n",
      "real maybe mind key book 1993 phone simple usually \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = talk.religion.misc\n",
      "\n",
      "going year life card 25 11 message short effect \n",
      "--------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# top 10 words with respect to theirs changable counter from the algorithm\n",
    "inverse_dict = {v:k  for k,v in vectorizer.vocabulary_.items()}\n",
    "\n",
    "for i in range(len(newsgroups_train.target_names)):\n",
    "    print('Top 10 in Topic = {0}'.format(newsgroups_train.target_names[i]))\n",
    "    print()\n",
    "    #x = np.argsort(num_topic_word[i])[:-10:-1]\n",
    "    x = np.argsort(num_topic_word[i]) [word_to_topic[np.argsort(num_topic_word[i])] == i] [:-10:-1]\n",
    "    for j in range(len(x)):\n",
    "        print(inverse_dict.get(x[j]), end = ' ')\n",
    "    print()\n",
    "    #print(x, sep = '\\n')\n",
    "    print('--------------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 in Topic = alt.atheism\n",
      "\n",
      "way believe atheism read doesn yes actually law accept \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = comp.graphics\n",
      "\n",
      "use just looking hi mail want gif send 24 \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = comp.os.ms-windows.misc\n",
      "\n",
      "file files drivers cica advance directory printer nt things \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = comp.sys.ibm.pc.hardware\n",
      "\n",
      "pc using dos scsi motherboard data cards problems bios \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = comp.sys.mac.hardware\n",
      "\n",
      "mac quadra disk speed simms price se internal info \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = comp.windows.x\n",
      "\n",
      "thanks mit user widget manager example trying software error \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = misc.forsale\n",
      "\n",
      "condition sell used 00 original brand power manuals windows \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = rec.autos\n",
      "\n",
      "car cars know think ford driver check went article \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = rec.motorcycles\n",
      "\n",
      "like dod ride right bikes riding motorcycle look thought \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = rec.sport.baseball\n",
      "\n",
      "baseball better league ll say home player braves mets \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = rec.sport.hockey\n",
      "\n",
      "game time games nhl players win rangers let ice \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = sci.crypt\n",
      "\n",
      "encryption clipper keys make nsa escrow means cryptography edu \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = sci.electronics\n",
      "\n",
      "need line low new voltage computer ground build series \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = sci.med\n",
      "\n",
      "does years geb skepticism intellect chastity shameful surrender cadre \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = sci.space\n",
      "\n",
      "work lunar 30 problem known lot given 93 office \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = soc.religion.christian\n",
      "\n",
      "don said fact man love lord sure matter heaven \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = talk.politics.guns\n",
      "\n",
      "people gun guns government did firearms course weapon defense \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = talk.politics.mideast\n",
      "\n",
      "serdar argic arabs armenia country turkey rights place occupied \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = talk.politics.misc\n",
      "\n",
      "public start taxes real general agree wrong evidence reason \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Top 10 in Topic = talk.religion.misc\n",
      "\n",
      "bible life word person going church making biblical fbi \n",
      "--------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# top 10 words with respect to theirs frequency in specific texts\n",
    "for i in range(len(newsgroups_train.target_names)):\n",
    "    print('Top 10 in Topic = {0}\\n'.format(newsgroups_train.target_names[i]))\n",
    "    x = np.argsort(beta[i]) [word_to_topic[np.argsort(beta[i])] == i] [:-10:-1]\n",
    "    for j in range(len(x)):\n",
    "        print(inverse_dict.get(x[j]), end = ' ')\n",
    "    print()\n",
    "    #print(x, sep = '\\n')\n",
    "    print('--------------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из представленных выше наборов слов алгоритм достаточно быстро (алгоритм произвел менее 200 итераций) ставит в соответствие каждому слову определенный тэг, который на самом деле соответствует основной тематике слова. Напротив, ~200 итераций явно недостотаточно для формирования распределения тэгов над словами, удовлетворящего критерию стабильности и нашему представлению о смысле слов. Это происходит в следствие того, что в алгоритме на каждом шаге счётчики меняются на 1, в то время как объём слов помноженный на количество тэгов (даже для усеченного словаря) слишком велик. Таким образом, для получения осмысленных результатов для распределения тэгов над словами необходимо провести намного больше итераций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примерное время выполнения шага алгоритма:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for count in range(10):                                            # will go more times through the corpus for stability\n",
    "    for i in range(len(newsgroups_train.data)):\n",
    "        text = newsgroups_train.data[i]\n",
    "        text_transformed = vectorizer.inverse_transform(vectorizer.transform([text]))[0]  # --[0] because of [text]\n",
    "        for j in range(len(text_transformed)):\n",
    "            # change counters\n",
    "            word = vectorizer.vocabulary_.get(text_transformed[j]) # word index in vocabulary_\n",
    "            topic = word_to_topic[word]\n",
    "            num_text_topic[i, topic] = num_text_topic[i, topic] - 1\n",
    "            num_topic[topic] = num_topic[topic] - 1\n",
    "            num_topic_word[topic, word] = num_topic_word[topic, word] - 1\n",
    "            #\n",
    "            p = np.zeros(len(num_topic))\n",
    "            for k in range(len(num_topic)):\n",
    "                # formula:  p[k] = (n_d,k + alpha[k]) * (n[k,w_i] + beta_w_i) / (n_k + beta_sum)\n",
    "                p[k] = (num_text_topic[i, k] + alpha[k]) * (num_topic_word[k, word] + beta[k, word]) / (num_topic[k] + \n",
    "                                                                                                        np.sum(beta[k]))\n",
    "                #\n",
    "            # sample new smth\n",
    "            topic = generate_with_weight(np.abs(p))\n",
    "            word_to_topic[word] = topic\n",
    "            # change counters back\n",
    "            num_text_topic[i, topic] = num_text_topic[i, topic] + 1\n",
    "            num_topic[topic] = num_topic[topic] + 1\n",
    "            num_topic_word[topic, word] = num_topic_word[topic, word] + 1\n",
    "            #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "word_to_topic = np.zeros(len(vectorizer.vocabulary_), dtype = int) # z     -- in which topic the word\n",
    "num_topic = np.zeros(len(newsgroups_train.target_names))           # n_k   -- number of words in topic k\n",
    "num_topic_word = np.zeros((len(newsgroups_train.target_names), len(vectorizer.vocabulary_)))   # <-- n_k,w\n",
    "                                                                   # n_k,w -- amount of times word w was assigned with topic k\n",
    "num_text_topic = np.zeros((len(newsgroups_train.data), len(newsgroups_train.target_names)))    # <-- n_d,k\n",
    "                                                                   # n_d,k -- number of words assigned with topic k in text d\n",
    "#num_corpus_topic                                                  # in algorithm NOT use n_d,k as d is all texts together\n",
    "\n",
    "alpha = np.zeros(len(newsgroups_train.target_names))               # alpha ~ distribution of topics over texts \n",
    "beta = np.zeros((len(newsgroups_train.target_names), len(vectorizer.vocabulary_)))  \n",
    "                                                                   # beta  ~ distribution of topics over words\n",
    "\n",
    "\n",
    "# random connect words with topics\n",
    "for i in range(len(vectorizer.vocabulary_)):       \n",
    "    # because len(newsgroups_train.target_names) =  20\n",
    "    word_to_topic[i] = generate_with_weight(np.full(20, 1/20))\n",
    "\n",
    "# update counters    \n",
    "for i in range(len(newsgroups_train.data)):\n",
    "    alpha[newsgroups_train.target[i]] = alpha[newsgroups_train.target[i]] + 1\n",
    "    text = newsgroups_train.data[i]\n",
    "    beta[newsgroups_train.target[i]] = beta[newsgroups_train.target[i]] + vectorizer.transform([text])\n",
    "    \n",
    "    x = np.resize(vectorizer.transform([text]).toarray(), len(vectorizer.vocabulary_))\n",
    "    b = np.argwhere(x)\n",
    "    c = word_to_topic[b]\n",
    "    for j in range(len(num_topic)):\n",
    "        num_text_topic[i, j] = len(c[(c == j)])\n",
    "        num_topic[j] = num_topic[j] + len(c[(c == j)])\n",
    "    text_transformed = vectorizer.inverse_transform(vectorizer.transform([text]))[0]\n",
    "    for j in range(len(text_transformed)):\n",
    "        word = vectorizer.vocabulary_.get(text_transformed[j])\n",
    "        num_topic_word[word_to_topic[word], word] = num_topic_word[word_to_topic[word], word] + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
